#!/bin/bash

rm -rf /ckpt
rm -rf /engine/config.json
rm -rf /engine/rank0.engine

# Write bash IF statement for arg --model == meta-llama_Llama-3.1-8B-Instruct
python3 /app/tensorrt_llm/examples/quantization/quantize.py \
--model_dir /model \
--dtype bfloat16   \
--qformat fp8   \
--kv_cache_dtype fp8   \
--output_dir /ckpt \
--pp_size 2 \
--calib_size 512

## the mkdir should use the --model arg
mkdir -p /engine/meta-llama_Llama-3.1-8B-Instruct
trtllm-build --checkpoint_dir /ckpt \
	--output_dir /engine/meta-llama_Llama-3.1-8B-Instruct   \
	--remove_input_padding enable \
	--kv_cache_type paged   \
	--max_batch_size 512   \
	--max_num_tokens 16355   \
	--max_seq_len 16355    \
	--use_paged_context_fmha enable   \
	--use_fp8_context_fmha enable   \
	--gemm_plugin disable   \
	--multiple_profiles enable

# Write bash IF statement for arg --model == Llama-3_3-Nemotron-Super-49B-v1-FP8
python3 /app/tensorrt_llm/examples/quantization/quantize.py \
--model_dir /model \
--dtype float16   \
--qformat fp8   \
--kv_cache_dtype fp8   \
--output_dir /ckpt \
--tp_size 2 \
--calib_size 512

## the mkdir should use the --model arg
mkdir -p /engine/Llama-3_3-Nemotron-Super-49B-v1-FP8

trtllm-build --checkpoint_dir /ckpt \
	--output_dir /engine/Llama-3_3-Nemotron-Super-49B-v1-FP8 \
	--remove_input_padding enable \
	--kv_cache_type paged   \
	--max_batch_size 512   \
	--max_num_tokens 16355   \
	--max_seq_len 16355    \
	--use_paged_context_fmha enable   \
	--use_fp8_context_fmha enable   \
	--gemm_plugin disable   \
	--multiple_profiles enable
